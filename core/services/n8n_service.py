import json
import logging
import requests
from django.conf import settings

logger = logging.getLogger(__name__)


def get_n8n_execution_status(execution_id):
    """Query n8n REST API for execution status and full data"""
    if not execution_id:
        return {'status': None, 'data': None}

    execution_api_url = settings.N8N_BASE_URL + '/api/v1/executions/' + str(execution_id) + '?includeData=true'
    headers = {'X-N8N-API-KEY': settings.N8N_API_KEY}

    try:
        response = requests.get(execution_api_url, headers=headers, timeout=5)
        if response.status_code == 200:
            data = response.json()
            return {'status': data.get('status', 'unknown'), 'data': data}
        elif response.status_code == 401:
            logger.error(f"Authentication failed for execution {execution_id}")
            return {'status': 'auth_error', 'data': None}
        else:
            logger.warning(f"Failed to get execution status for {execution_id}: HTTP {response.status_code}")
            return {'status': 'unknown', 'data': None}
    except requests.exceptions.Timeout:
        logger.warning(f"Timeout getting execution status for {execution_id}")
        return {'status': 'timeout', 'data': None}
    except Exception as e:
        logger.error(f"Exception getting execution status for {execution_id}: {str(e)}")
        return {'status': 'error', 'data': None}


def build_source_config(cleaned_data):
    """Convert Django form data to n8n configuration format"""
    source_type = cleaned_data['source_type']
    config = {}
    
    # Helper function to safely get string value and split into lines
    def get_string_list(field_name, default=''):
        value = cleaned_data.get(field_name, default)
        if isinstance(value, list):
            # If it's already a list, return it as-is
            return [str(item).strip() for item in value if str(item).strip()]
        elif isinstance(value, str):
            # If it's a string, split by lines
            return [line.strip() for line in value.splitlines() if line.strip()]
        else:
            # Convert to string and split
            return [str(value).strip() for line in str(value).splitlines() if line.strip()]
    
    if source_type.startswith('youtube-'):
        max_results = cleaned_data.get('max_results', 50)
        
        # Common fields for all YouTube source types
        config['maxResults'] = max_results
        config['maxResultsShorts'] = 0  # No shorts for testing
        config['maxResultStreams'] = 0  # No streams for testing
        
        if source_type == 'youtube-search':
            config['searchQueries'] = get_string_list('search_queries')
            config['sortingOrder'] = cleaned_data.get('sorting_order', 'relevance')
            config['dateFilter'] = cleaned_data.get('date_filter', 'month')
            config['videoType'] = cleaned_data.get('video_type', 'video')
            config['lengthFilter'] = cleaned_data.get('length_filter', 'between420')
        else:
            # YouTube channel, playlist, hashtag, video
            urls = get_string_list('profile_urls')
            config['startUrls'] = [{'url': url} for url in urls]
            # Add these fields for non-search types too (for consistency)
            config['sortingOrder'] = cleaned_data.get('sorting_order', 'date')
            config['dateFilter'] = cleaned_data.get('date_filter', 'month')
            config['videoType'] = cleaned_data.get('video_type', 'video')
            config['lengthFilter'] = cleaned_data.get('length_filter', 'between420')
        
        # Complete quality filters (matching workflow pinData)
        config['qualityFilters'] = {
            'isHD': cleaned_data.get('is_hd', False),
            'hasSubtitles': cleaned_data.get('has_subtitles', False),
            'hasCC': cleaned_data.get('has_cc', False),
            'is3D': cleaned_data.get('is_3d', False),
            'isLive': cleaned_data.get('is_live', False),
            'is4K': cleaned_data.get('is_4k', False),
            'is360': cleaned_data.get('is_360', False),
            'hasLocation': cleaned_data.get('has_location', False),
            'isHDR': cleaned_data.get('is_hdr', False),
            'isVR180': cleaned_data.get('is_vr180', False),
            'isBought': cleaned_data.get('is_bought', False),
        }
        
        # Complete subtitle options (matching workflow pinData)
        config['subtitleOptions'] = {
            'language': cleaned_data.get('subtitles_language', 'en'),
            'downloadSubtitles': cleaned_data.get('download_subtitles', False),
            'saveSubsToKVS': cleaned_data.get('save_subs_to_kvs', False),
            'preferAutoGeneratedSubtitles': cleaned_data.get('prefer_auto_generated_subtitles', False),
            'subtitlesFormat': 'plaintext',
        }
        
        # Also add top-level subtitle fields for Apify compatibility
        config['downloadSubtitles'] = cleaned_data.get('download_subtitles', False)
        config['saveSubsToKVS'] = cleaned_data.get('save_subs_to_kvs', False)
        config['subtitlesLanguage'] = cleaned_data.get('subtitles_language', 'en')
        config['preferAutoGeneratedSubtitles'] = cleaned_data.get('prefer_auto_generated_subtitles', False)
        config['subtitlesFormat'] = 'plaintext'
        
        # Force required defaults for YouTube fields if they are empty/missing
        if not config.get('dateFilter'):
            config['dateFilter'] = 'month'
        if not config.get('lengthFilter'):
            config['lengthFilter'] = 'between420'
        if not config.get('videoType'):
            config['videoType'] = 'video'
    
    elif source_type.startswith('instagram-'):
        if source_type == 'instagram-search':
            config['search'] = get_string_list('search_queries')
        else:
            config['directUrls'] = get_string_list('profile_urls')
        
        config['resultsType'] = cleaned_data.get('results_type', 'posts')
        config['resultsLimit'] = cleaned_data.get('max_results', 100)
        config['oldestPostDate'] = cleaned_data.get('oldest_post_date', '')
        config['relativeDateFilter'] = cleaned_data.get('relative_date_filter', '')
        config['feedType'] = cleaned_data.get('feed_type', 'posts')
    
    elif source_type.startswith('tiktok-'):
        if source_type == 'tiktok-profile':
            config['profiles'] = get_string_list('profile_urls')
        elif source_type == 'tiktok-hashtag':
            config['hashtags'] = get_string_list('hashtags')
        elif source_type == 'tiktok-search':
            config['searchQueries'] = get_string_list('search_queries')
        elif source_type == 'tiktok-video':
            config['postURLs'] = get_string_list('profile_urls')
        
        config['resultsPerPage'] = cleaned_data.get('max_results', 50)
    
    return config


def trigger_run(run):
    """Trigger multi-source n8n workflow"""
    # Parse input
    input_data = json.loads(run.input)
    sources = input_data.get('sources', [])
    days_since = input_data.get('days_since', 14)
    max_results = input_data.get('max_results', 50)
    auto_infer_columns = input_data.get('auto_infer_columns', True)
    custom_columns = input_data.get('custom_columns', [])
    extraction_prompt = input_data.get('extraction_prompt', "Extract location information, business mentions, contact details, and other relevant data from social media posts. Adapt to the specific platform and content type.")
    enable_extraction = input_data.get('enable_extraction', True)

    # Convert new sourceType format to platform format for n8n compatibility
    converted_sources = []
    for source in sources:
        source_type = source.get('sourceType', '')
        config = source.get('config', {})
        
        # Map sourceType to platform
        if source_type.startswith('youtube-'):
            platform = 'youtube'
        elif source_type.startswith('instagram-'):
            platform = 'instagram'
        elif source_type.startswith('tiktok-'):
            platform = 'tiktok'
        else:
            platform = 'instagram'  # Default fallback
        
        # Create platform-specific source object
        platform_source = {
            'platform': platform,
            'sourceType': source_type,
            'config': config
        }
        
        # Add legacy fields for backward compatibility
        if platform == 'instagram':
            if 'directUrls' in config:
                platform_source['directUrls'] = config['directUrls']
            elif 'search' in config:
                platform_source['search'] = config['search']
        elif platform == 'tiktok':
            if 'profiles' in config:
                platform_source['profiles'] = config['profiles']
            if 'hashtags' in config:
                platform_source['hashtags'] = config['hashtags']
            if 'searchQueries' in config:
                platform_source['searchQueries'] = config['searchQueries']
            if 'postURLs' in config:
                platform_source['postURLs'] = config['postURLs']
        elif platform == 'youtube':
            if 'startUrls' in config:
                platform_source['startUrls'] = config['startUrls']
            if 'searchQueries' in config:
                platform_source['searchQueries'] = config['searchQueries']
        
        converted_sources.append(platform_source)

    # Format payload for multi-source n8n workflow
    payload = {
        "run_id": run.pk,
        "customer_id": run.user_id,
        "sources": converted_sources,
        "auto_infer_columns": auto_infer_columns,
        "custom_columns": custom_columns,
        "enable_extraction": enable_extraction,
        "extraction_prompt": extraction_prompt
    }
    
    # Use production multi-source URL
    multi_source_url = settings.N8N_MULTI_SOURCE_URL
    
    # Prepare basic auth credentials
    auth = (settings.N8N_BASIC_AUTH_USER, settings.N8N_BASIC_AUTH_PASSWORD)
    
    try:
        logger.info(f"Triggering multi-source n8n workflow for run {run.pk} at {multi_source_url}")
        logger.info(f"Payload: {json.dumps(payload, indent=2)}")
        
        response = requests.post(multi_source_url, json=payload, auth=auth, timeout=30)
        
        if response.status_code == 200:
            # Try to parse execution_id from response
            try:
                response_data = response.json()
                run.n8n_execution_id = response_data.get('n8n_execution_id')
            except (ValueError, KeyError):
                # If response is empty or invalid, get latest execution from API
                logger.info(f"Empty response from webhook, querying n8n API for latest execution")
                try:
                    executions_url = settings.N8N_BASE_URL + '/api/v1/executions?workflowId=XKmvY7OoI2VLpAem&limit=1'
                    api_response = requests.get(executions_url, 
                        headers={'X-N8N-API-KEY': settings.N8N_API_KEY}, 
                        timeout=10)
                    if api_response.status_code == 200:
                        executions_data = api_response.json()
                        if executions_data.get('data'):
                            run.n8n_execution_id = executions_data['data'][0]['id']
                except Exception as api_error:
                    logger.warning(f"Failed to get execution ID from API: {api_error}")
            
            run.save()
            logger.info(f"Run {run.pk} started with execution {run.n8n_execution_id}")
        else:
            logger.error(f"Failed to start run {run.pk}: HTTP {response.status_code} - {response.text}")
            # Try fallback to legacy webhook if multi-source fails
            try_legacy_fallback(run, input_data)
    except Exception as e:
        logger.error(f"Exception while triggering run {run.pk}: {str(e)}")
        # Try fallback to legacy webhook
        try_legacy_fallback(run, input_data)


def try_legacy_fallback(run, input_data):
    """Fallback to legacy single-source workflow if multi-source fails"""
    try:
        logger.info(f"Attempting fallback to legacy workflow for run {run.pk}")
        
        # Extract first source for fallback
        sources = input_data.get('sources', [])
        if not sources:
            logger.error("No sources available for fallback")
            return
            
        first_source = sources[0]
        source_type = first_source.get('sourceType', '')
        config = first_source.get('config', {})
        
        # Map sourceType to platform
        if source_type.startswith('youtube-'):
            platform = 'youtube'
        elif source_type.startswith('instagram-'):
            platform = 'instagram'
        elif source_type.startswith('tiktok-'):
            platform = 'tiktok'
        else:
            platform = 'instagram'  # Default fallback
        
        # Extract URLs based on source type and config
        profiles = []
        if platform == 'instagram':
            profiles = config.get('directUrls', [])
        elif platform == 'tiktok':
            profiles = config.get('profiles', [])
            # For TikTok video, use postURLs
            if not profiles and 'postURLs' in config:
                profiles = config.get('postURLs', [])
            # For TikTok search, use search queries as profiles for legacy compatibility
            if not profiles and 'searchQueries' in config:
                profiles = config.get('searchQueries', [])
        elif platform == 'youtube':
            # For YouTube, try different URL types
            if 'startUrls' in config:
                profiles = [url.get('url', '') if isinstance(url, dict) else url for url in config['startUrls']]
            elif 'searchQueries' in config:
                # For search, use search queries as profiles for legacy compatibility
                profiles = config.get('searchQueries', [])
        
        if not profiles:
            logger.error(f"No profiles/URLs found for {platform} source in legacy fallback")
            return
        
        days_since = input_data.get('days_since', 14)
        max_results = input_data.get('max_results', 50)
        extraction_prompt = input_data.get('extraction_prompt', "Extract location information, business mentions, and contact details from social media posts.")
        
        # Format legacy payload
        n8n_profiles = []
        for url in profiles:
            if url and url.strip():
                n8n_profiles.append({
                    "url": url.strip(),
                    "type": platform
                })
        
        if not n8n_profiles:
            logger.error("No valid profiles to process in legacy fallback")
            return
        
        legacy_payload = {
            "user_id": run.user_id,
            "tier": "premium",
            "days_since": days_since,
            "max_results": max_results,
            "extraction_prompt": extraction_prompt,
            "profiles": n8n_profiles
        }
        
        # Post to legacy n8n webhook with authentication
        webscrape_url = settings.N8N_WEBSCRAPE_URL
        auth = (settings.N8N_BASIC_AUTH_USER, settings.N8N_BASIC_AUTH_PASSWORD)
        response = requests.post(webscrape_url, json=legacy_payload, auth=auth, timeout=10)
        
        if response.status_code == 200:
            response_data = response.json()
            run.n8n_execution_id = response_data.get('execution_id')
            run.save()
            logger.info(f"Run {run.pk} started with legacy execution {run.n8n_execution_id}")
        else:
            logger.error(f"Legacy fallback also failed for run {run.pk}: HTTP {response.status_code} - {response.text}")
            
    except Exception as e:
        logger.error(f"Exception in legacy fallback for run {run.pk}: {str(e)}")