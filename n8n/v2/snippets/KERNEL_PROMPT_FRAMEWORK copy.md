# Framework

K - Keep it simple

Bad: 500 words of context

Good: One clear goal

Example: Instead of "I need help writing something about Redis," use "Write a technical tutorial on Redis caching"

Result: 70% less token usage, 3x faster responses

E - Easy to verify

Your prompt needs clear success criteria

Replace "make it engaging" with "include 3 code examples"

If you can't verify success, AI can't deliver it

My testing: 85% success rate with clear criteria vs 41% without

R - Reproducible results

Avoid temporal references ("current trends", "latest best practices")

Use specific versions and exact requirements

Same prompt should work next week, next month

94% consistency across 30 days in my tests

N - Narrow scope

One prompt = one goal

Don't combine code + docs + tests in one request

Split complex tasks

Single-goal prompts: 89% satisfaction vs 41% for multi-goal

E - Explicit constraints

Tell AI what NOT to do

"Python code" â†’ "Python code. No external libraries. No functions over 20 lines."

Constraints reduce unwanted outputs by 91%

L - Logical structure Format every prompt like:

Context (input)

Task (function)

Constraints (parameters)

Format (output)

**credit:**
- [post: After 1000 hours of prompt engineering, I found the 6 patterns that actually matter](https://www.reddit.com/r/PromptEngineering/comments/1nt7x7v/after_1000_hours_of_prompt_engineering_i_found/) 
- [user: volodith](https://www.reddit.com/user/volodith/)
